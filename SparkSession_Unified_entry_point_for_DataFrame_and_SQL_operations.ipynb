{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPz9I9Wx80SBluu8TFm1X6K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IsfaquethedataAnalyst/Data_Analyst/blob/main/SparkSession_Unified_entry_point_for_DataFrame_and_SQL_operations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4pmt6ADrCsYu",
        "outputId": "c680ca45-6a92-40bc-fe12-35bfafbf73f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+\n",
            "|   Name|Age|\n",
            "+-------+---+\n",
            "|  Alice| 25|\n",
            "|    Bob| 30|\n",
            "|Charlie| 35|\n",
            "+-------+---+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"PySpark Basics\").getOrCreate()\n",
        "\n",
        "# Create a DataFrame\n",
        "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# From a list of tuples\n",
        "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
        "\n",
        "# From a Pandas DataFrame\n",
        "import pandas as pd\n",
        "pandas_df = pd.DataFrame({\"Name\": [\"Alice\", \"Bob\", \"Charlie\"], \"Age\": [25, 30, 35]})\n",
        "spark_df = spark.createDataFrame(pandas_df)"
      ],
      "metadata": {
        "id": "8xJJ6P-dDfVC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the DataFrame\n",
        "df.show()\n",
        "\n",
        "# Print the schema\n",
        "df.printSchema()\n",
        "\n",
        "# Select specific columns\n",
        "df.select(\"Name\").show()\n",
        "\n",
        "# Filter rows\n",
        "df.filter(df.Age > 30).show()\n",
        "\n",
        "# Add a new column\n",
        "from pyspark.sql.functions import col\n",
        "df = df.withColumn(\"AgeNextYear\", col(\"Age\") + 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZI1_ctkFMNr",
        "outputId": "a4b1d2cd-7c26-4cbb-da78-e2a1bf95e986"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+\n",
            "|   Name|Age|\n",
            "+-------+---+\n",
            "|  Alice| 25|\n",
            "|    Bob| 30|\n",
            "|Charlie| 35|\n",
            "+-------+---+\n",
            "\n",
            "root\n",
            " |-- Name: string (nullable = true)\n",
            " |-- Age: long (nullable = true)\n",
            "\n",
            "+-------+\n",
            "|   Name|\n",
            "+-------+\n",
            "|  Alice|\n",
            "|    Bob|\n",
            "|Charlie|\n",
            "+-------+\n",
            "\n",
            "+-------+---+\n",
            "|   Name|Age|\n",
            "+-------+---+\n",
            "|Charlie| 35|\n",
            "+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.createOrReplaceTempView(\"people\")"
      ],
      "metadata": {
        "id": "6PsIQopHFZDT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = spark.sql(\"SELECT * FROM people WHERE Age > 30\")\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOvlHqvWFhkJ",
        "outputId": "4643d0d0-c3a3-4fba-8738-615a045c9571"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+-----------+\n",
            "|   Name|Age|AgeNextYear|\n",
            "+-------+---+-----------+\n",
            "|Charlie| 35|         36|\n",
            "+-------+---+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg\n",
        "avg_age = df.select(avg(\"Age\")).collect()[0][0]\n",
        "print(f\"Average age: {avg_age}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sB4m-KHmFqG3",
        "outputId": "734c6cce-4b8d-4d0a-d86e-bb39caf6e929"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average age: 30.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number, dense_rank, rank\n",
        "\n",
        "# Assuming you want to partition by 'Name' and order by 'Age' as a proxy\n",
        "windowSpec = Window.partitionBy(\"Name\").orderBy(\"Age\")\n",
        "\n",
        "df = df.withColumn(\"row_number\", row_number().over(windowSpec)) \\\n",
        "       .withColumn(\"dense_rank\", dense_rank().over(windowSpec)) \\\n",
        "       .withColumn(\"rank\", rank().over(windowSpec))\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IYrAoPN9GX7h",
        "outputId": "c83be72b-5af9-48a6-801c-591b74c40619"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+-----------+----------+----------+----+\n",
            "|   Name|Age|AgeNextYear|row_number|dense_rank|rank|\n",
            "+-------+---+-----------+----------+----------+----+\n",
            "|  Alice| 25|         26|         1|         1|   1|\n",
            "|    Bob| 30|         31|         1|         1|   1|\n",
            "|Charlie| 35|         36|         1|         1|   1|\n",
            "+-------+---+-----------+----------+----------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import row_number, dense_rank, rank\n",
        "\n",
        "# Assuming 'Name' and 'Age' are the relevant columns for your ranking\n",
        "windowSpec = Window.partitionBy(\"Name\").orderBy(\"Age\")\n",
        "\n",
        "df = df.withColumn(\"row_number\", row_number().over(windowSpec)) \\\n",
        "       .withColumn(\"dense_rank\", dense_rank().over(windowSpec)) \\\n",
        "       .withColumn(\"rank\", rank().over(windowSpec))"
      ],
      "metadata": {
        "id": "9RJW94UkPj3e"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum, avg, max, min, count\n",
        "\n",
        "# Verify the column names in your DataFrame\n",
        "print(df.columns)\n",
        "\n",
        "# Choose the correct column name to group by.\n",
        "# If there's no column to group by, remove the groupBy clause entirely.\n",
        "result = df.groupBy(\"Name\") \\\n",
        "           .agg(sum(\"Age\").alias(\"total_age\"),  # Replace 'Age' with the column you want to aggregate\n",
        "                avg(\"Age\").alias(\"avg_age\"),\n",
        "                max(\"Age\").alias(\"max_age\"), # Fixed indentation\n",
        "                min(\"Age\").alias(\"min_age\"),\n",
        "                count(\"*\").alias(\"count\"))\n",
        "\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LM_GACzPDd9",
        "outputId": "ed2366e0-a291-4cfc-8492-89395756d990"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Name', 'Age', 'AgeNextYear', 'row_number', 'dense_rank', 'rank', 'age_category']\n",
            "+-------+---------+-------+-------+-------+-----+\n",
            "|   Name|total_age|avg_age|max_age|min_age|count|\n",
            "+-------+---------+-------+-------+-------+-----+\n",
            "|  Alice|       25|   25.0|     25|     25|    1|\n",
            "|Charlie|       35|   35.0|     35|     35|    1|\n",
            "|    Bob|       30|   30.0|     30|     30|    1|\n",
            "+-------+---------+-------+-------+-------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "employees = spark.createDataFrame([(\"Alice\", \"Sales\"), (\"Bob\", \"Engineering\")], [\"name\", \"department\"])\n",
        "departments = spark.createDataFrame([(\"Sales\", \"New York\"), (\"Engineering\", \"San Francisco\")], [\"department\", \"location\"])\n",
        "\n",
        "result = employees.join(departments, \"department\")"
      ],
      "metadata": {
        "id": "NIr30PM9PG8P"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "def age_category(age):\n",
        "    return 1 if age < 30 else 2 if age < 50 else 3\n",
        "\n",
        "age_category_udf = udf(age_category, IntegerType())\n",
        "\n",
        "df = df.withColumn(\"age_category\", age_category_udf(df.Age))"
      ],
      "metadata": {
        "id": "DSSMQW7VPIcM"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import pandas_udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "@pandas_udf(IntegerType())\n",
        "def age_category(age: pd.Series) -> pd.Series:\n",
        "    return pd.cut(age, bins=[0, 30, 50, 150], labels=[1, 2, 3], include_lowest=True)\n",
        "\n",
        "df = df.withColumn(\"age_category\", age_category(df.Age))"
      ],
      "metadata": {
        "id": "A51taJ5sPNBt"
      },
      "execution_count": 33,
      "outputs": []
    }
  ]
}